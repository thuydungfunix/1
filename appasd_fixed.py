
# -*- coding: utf-8 -*-
import streamlit as st
import numpy as np
import pandas as pd
import csv
import glob
import os
from pathlib import Path
from catboost import CatBoostClassifier
from sklearn.preprocessing import LabelEncoder

st.set_page_config(page_title="SÃ ng Lá»c Tá»± Ká»· - Fixed", page_icon="ğŸ§ ", layout="centered")

# ------------------------- Utilities -------------------------
def find_model_file():
    # try common names then glob for .cbm in cwd and /mnt/data
    candidates = ["catboost_asd.cbm", "catboost_model.cbm", "model.cbm", "catboost.cbm"]
    for c in candidates:
        if Path(c).exists():
            return str(Path(c))
        if Path("/mnt/data") / c:
            p = Path("/mnt/data") / c
            if p.exists():
                return str(p)
    # glob search
    for d in [".", "/mnt/data"]:
        for f in glob.glob(os.path.join(d, "*.cbm")):
            return f
    return None

@st.cache_resource
def load_model():
    model_path = find_model_file()
    if model_path is None:
        st.warning("KhÃ´ng tÃ¬m tháº¥y file model (.cbm). Vui lÃ²ng Ä‘áº·t file model trong thÆ° má»¥c cháº¡y app (vÃ­ dá»¥: catboost_model.cbm).")
        return None
    model = CatBoostClassifier()
    model.load_model(model_path)
    return model

def load_arff(path="Autism-Adult-Data.arff"):
    """
    Minimal ARFF parser to get attribute names and the dataset as a pandas DataFrame.
    Expects that the ARFF file structure is similar to the provided file.
    """
    p = Path(path)
    if not p.exists():
        return None, None
    lines = p.read_text(encoding="utf-8", errors="ignore").splitlines()
    attrs = []
    data_start = None
    for i,l in enumerate(lines):
        ls = l.strip()
        if ls.lower().startswith('@attribute'):
            # get attribute name (second token)
            try:
                parts = ls.split()
                name = parts[1]
            except:
                continue
            attrs.append(name.strip())
        if ls.lower().startswith('@data'):
            data_start = i+1
            break
    # read data lines with csv.reader to handle quoted values
    data_lines = lines[data_start:]
    reader = csv.reader(data_lines, quotechar="'", skipinitialspace=True)
    rows = list(reader)
    # if number of columns matches attributes, create dataframe
    if len(rows)>0 and len(rows[0]) == len(attrs):
        df = pd.DataFrame(rows, columns=attrs)
        # strip quotes/spaces
        df = df.applymap(lambda x: x.strip().strip("'") if isinstance(x,str) else x)
        # convert numeric where appropriate
        for col in df.columns:
            if col.startswith('A') or col in ['age','result']:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        return attrs, df
    else:
        return attrs, None

# ------------------------- Prepare model + encoders -------------------------
model = load_model()

attrs, df_dataset = load_arff("Autism-Adult-Data.arff")
if attrs is None:
    attrs = []

# We assume the original model was trained on the first 16 attributes in the ARFF file (A1..contry_of_res)
# This matches the uploaded dataset structure used when fixing the app.
feature_attrs = attrs[:16] if len(attrs) >= 16 else None

# build label encoders for categorical columns used by the model (based on ARFF)
cat_cols = []
encoders = {}
if df_dataset is not None and feature_attrs is not None:
    # identify categorical columns among the first 16 (non-numeric)
    for c in feature_attrs:
        if df_dataset[c].dtype == object:
            cat_cols.append(c)
    # fit encoders (LabelEncoder behaves like typical training pipelines that encode categories to ints)
    for c in cat_cols:
        le = LabelEncoder()
        vals = df_dataset[c].fillna("NA").astype(str).tolist()
        le.fit(vals)
        encoders[c] = le

# ------------------------- UI -------------------------
st.title("ğŸ” á»¨ng dá»¥ng SÃ ng Lá»c Tá»± Ká»· (ASD) â€” Fixed")
st.markdown("á»¨ng dá»¥ng nÃ y dÃ¹ng model CatBoost (file `.cbm`) vÃ  Ã¡nh xáº¡ cÃ¡c Ä‘áº·c trÆ°ng dá»±a trÃªn bá»™ dá»¯ liá»‡u ARFF Ä‘Ã£ táº£i lÃªn.")

st.header("ğŸ“ Vui lÃ²ng tráº£ lá»i cÃ¡c cÃ¢u há»i sau:")

# 10 cÃ¢u há»i A1 - A10 (0/1)
questions = [
    "1. CÃ³ khi nÃ o ngÆ°á»i Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃ¡nh giao tiáº¿p báº±ng máº¯t?",
    "2. NgÆ°á»i Ä‘Ã³ cÃ³ thÃ­ch chÆ¡i má»™t mÃ¬nh?",
    "3. NgÆ°á»i Ä‘Ã³ cÃ³ hay láº·p láº¡i tá»«/ngÃ´n ngá»¯ khÃ´ng?",
    "4. NgÆ°á»i Ä‘Ã³ cÃ³ khÃ³ khÄƒn khi hiá»ƒu cáº£m xÃºc ngÆ°á»i khÃ¡c?",
    "5. CÃ³ khi nÃ o ngÆ°á»i Ä‘Ã³ khÃ´ng pháº£n há»“i khi Ä‘Æ°á»£c gá»i tÃªn?",
    "6. NgÆ°á»i Ä‘Ã³ cÃ³ nháº¡y cáº£m vá»›i Ã¢m thanh khÃ´ng?",
    "7. CÃ³ khi nÃ o ngÆ°á»i Ä‘Ã³ khÃ´ng chia sáº» há»©ng thÃº hoáº·c thÃ nh tÃ­ch vá»›i ngÆ°á»i khÃ¡c?",
    "8. NgÆ°á»i Ä‘Ã³ cÃ³ hÃ nh vi láº·p Ä‘i láº·p láº¡i khÃ´ng?",
    "9. NgÆ°á»i Ä‘Ã³ cÃ³ gáº·p khÃ³ khÄƒn khi thay Ä‘á»•i thÃ³i quen hoáº·c mÃ´i trÆ°á»ng khÃ´ng?",
    "10. CÃ³ khi nÃ o ngÆ°á»i Ä‘Ã³ khÃ´ng hiá»ƒu cÃ¡c quy táº¯c xÃ£ há»™i cÆ¡ báº£n khÃ´ng?"
]
a_scores = []
for i,q in enumerate(questions):
    ans = st.selectbox(q, ["KhÃ´ng", "CÃ³"], key=f"A_{i+1}")
    a_scores.append(1 if ans == "CÃ³" else 0)

# CÃ¡c Ä‘áº·c trÆ°ng khÃ¡c (nhá»¯ng thuá»™c tÃ­nh Ä‘áº§u tiÃªn trong dataset)
age = st.number_input("11. Tuá»•i cá»§a ngÆ°á»i Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡:", min_value=1, max_value=120, value=18)
gender_choice = st.selectbox("12. Giá»›i tÃ­nh:", ["Nam", "Ná»¯"])

# If dataset was loaded, provide selection lists for ethnicity and country using values from the ARFF.
if df_dataset is not None:
    ethnicity_options = sorted(df_dataset['ethnicity'].fillna("Unknown").unique().tolist())
    country_options = sorted(df_dataset['contry_of_res'].fillna("Unknown").unique().tolist())
else:
    # Fallback defaults (these should be replaced by your dataset if available)
    ethnicity_options = ["White-European", "Latino", "Others", "Black", "Asian", "Hispanic", "South Asian"]
    country_options = ["United States", "India", "Brazil", "Spain", "Egypt", "Sri Lanka", "New Zealand"]

ethnicity = st.selectbox("13. Ethnicity (dÃ¢n tá»™c):", ethnicity_options)
jundice = st.selectbox("14. NgÆ°á»i Ä‘Ã³ cÃ³ bá»‹ vÃ ng da sau sinh khÃ´ng?", ["KhÃ´ng", "CÃ³"])
austim = st.selectbox("15. Gia Ä‘Ã¬nh cÃ³ ngÆ°á»i tá»«ng bá»‹ tá»± ká»· khÃ´ng?", ["KhÃ´ng", "CÃ³"])
# These two fields exist in dataset but may not be part of the model's features â€” keep for UI only
used_app_before = st.selectbox("16. Báº¡n Ä‘Ã£ tá»«ng sá»­ dá»¥ng á»©ng dá»¥ng nÃ y chÆ°a?", ["ChÆ°a", "Rá»“i"])
relation = st.selectbox("17. Báº¡n lÃ  ai Ä‘á»‘i vá»›i ngÆ°á»i Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡?", ["Bá»‘", "Máº¹", "Báº£n thÃ¢n", "NgÆ°á»i thÃ¢n khÃ¡c"])

# ------------------------- Build features for prediction -------------------------
def build_input_dataframe(a_scores, age, gender_choice, ethnicity, jundice, austim, country, feature_attrs, encoders):
    if feature_attrs is None:
        st.error("KhÃ´ng cÃ³ thÃ´ng tin cáº¥u trÃºc Ä‘áº·c trÆ°ng (feature attributes). Vui lÃ²ng kiá»ƒm tra file ARFF.")
        return None
    row = {}
    # A1..A10
    for i in range(10):
        row[feature_attrs[i]] = int(a_scores[i])
    # age
    if 'age' in feature_attrs:
        row['age'] = int(age)
    # gender: map VN -> dataset values ('m'/'f')
    gender_map = {'Nam': 'm', 'Ná»¯': 'f'}
    if 'gender' in feature_attrs:
        val = gender_map.get(gender_choice, gender_choice)
        if 'gender' in encoders:
            row['gender'] = int(encoders['gender'].transform([str(val)])[0])
        else:
            # fallback: try numeric
            row['gender'] = 1 if val == 'm' else 0
    # ethnicity
    if 'ethnicity' in feature_attrs:
        if 'ethnicity' in encoders:
            row['ethnicity'] = int(encoders['ethnicity'].transform([str(ethnicity)])[0])
        else:
            row['ethnicity'] = str(ethnicity)
    # jundice
    if 'jundice' in feature_attrs:
        val = 'yes' if jundice == 'CÃ³' else 'no'
        if 'jundice' in encoders:
            row['jundice'] = int(encoders['jundice'].transform([str(val)])[0])
        else:
            row['jundice'] = val
    # austim (family history)
    if 'austim' in feature_attrs:
        val = 'yes' if austim == 'CÃ³' else 'no'
        if 'austim' in encoders:
            row['austim'] = int(encoders['austim'].transform([str(val)])[0])
        else:
            row['austim'] = val
    # country (contry_of_res)
    if 'contry_of_res' in feature_attrs:
        if 'contry_of_res' in encoders:
            row['contry_of_res'] = int(encoders['contry_of_res'].transform([str(country)])[0])
        else:
            row['contry_of_res'] = str(country)
    # If some attributes from feature_attrs were not set above, fill with zeros
    for c in feature_attrs:
        if c not in row:
            # default numeric 0
            row[c] = 0
    # build dataframe ordered
    df_in = pd.DataFrame([[row[c] for c in feature_attrs]], columns=feature_attrs)
    return df_in

# Prediction button
if st.button("ğŸ“Š Dá»± Ä‘oÃ¡n kháº£ nÄƒng tá»± ká»·"):
    if model is None:
        st.error("Model chÆ°a Ä‘Æ°á»£c táº£i. HÃ£y Ä‘áº£m báº£o file .cbm cÃ³ trong thÆ° má»¥c cháº¡y app.")
    else:
        df_in = build_input_dataframe(a_scores, age, gender_choice, ethnicity, jundice, austim, country_options[0] if len(country_options)>0 else 'Unknown', feature_attrs, encoders)
        # Note: above we accidentally used country_options[0] as default; override with selected country
        # Rebuild with real selected country
        df_in = build_input_dataframe(a_scores, age, gender_choice, ethnicity, jundice, austim, country, feature_attrs, encoders)
        if df_in is None:
            st.error("KhÃ´ng thá»ƒ khá»Ÿi táº¡o dá»¯ liá»‡u Ä‘áº§u vÃ o.")
        else:
            try:
                probs = model.predict_proba(df_in)
                # get index of class '1' (positive label) if present in model.classes_
                classes = list(model.classes_)
                try:
                    idx_pos = classes.index(1.0)
                except ValueError:
                    # fallback: take second column
                    idx_pos = 1 if probs.shape[1] > 1 else 0
                proba = float(probs[0][idx_pos])
                pred = int(probs[0][idx_pos] >= 0.5)
                st.subheader("ğŸ” Káº¿t quáº£ sÃ ng lá»c:")
                st.write(f"ğŸ‘‰ XÃ¡c suáº¥t máº¯c tá»± ká»· (ASD): **{proba:.2f}**")
                st.markdown("### ğŸ§­ Gá»£i Ã½ hÃ nh Ä‘á»™ng tiáº¿p theo:")
                if pred == 1:
                   st.error("""
âš ï¸ Nguy cÆ¡ cao máº¯c ASD.
ğŸ”¹ HÃ£y liÃªn há»‡ chuyÃªn gia tÃ¢m lÃ½ hoáº·c cÆ¡ sá»Ÿ y táº¿ Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n ká»¹ lÆ°á»¡ng hÆ¡n.
ğŸ”¹ Ghi chÃ©p láº¡i cÃ¡c biá»ƒu hiá»‡n thÆ°á»ng gáº·p trong cuá»™c sá»‘ng hÃ ng ngÃ y.
ğŸ”¹ CÃ³ thá»ƒ tham kháº£o tÃ i liá»‡u vá» ASD tá»« WHO, CDC hoáº·c cÃ¡c trung tÃ¢m há»— trá»£ trong nÆ°á»›c.
""")

                else:
                  st.success("""
âœ… Nguy cÆ¡ tháº¥p máº¯c ASD.
ğŸ”¹ Báº¡n cÃ³ thá»ƒ yÃªn tÃ¢m á»Ÿ thá»i Ä‘iá»ƒm hiá»‡n táº¡i.
ğŸ”¹ Náº¿u váº«n cÃ²n bÄƒn khoÄƒn, hÃ£y trao Ä‘á»•i thÃªm vá»›i chuyÃªn gia.
""")

            except Exception as e:
                st.error(f"ÄÃ£ xáº£y ra lá»—i khi dá»± Ä‘oÃ¡n: {e}")

st.markdown(\"---\")
st.caption(\"Ghi chÃº: á»¨ng dá»¥ng nÃ y sá»­ dá»¥ng file model CatBoost (.cbm) vÃ  Ã¡nh xáº¡ cÃ¡c Ä‘áº·c trÆ°ng báº±ng LabelEncoder dá»±a trÃªn bá»™ dá»¯ liá»‡u ARFF cung cáº¥p. Náº¿u báº¡n muá»‘n thay Ä‘á»•i pipeline tiá»n xá»­ lÃ½ (vÃ­ dá»¥: giá»¯ relation, used_app_before), vui lÃ²ng gá»­i file code training Ä‘á»ƒ chÃºng tÃ´i tÃ¡i táº¡o chÃ­nh xÃ¡c quy trÃ¬nh tiá»n xá»­ lÃ½.\") 
